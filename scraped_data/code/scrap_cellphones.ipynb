{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-31T00:03:37.419265Z",
     "start_time": "2021-10-31T00:03:37.413023Z"
    }
   },
   "source": [
    "# Scrap CellphoneS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T12:34:16.857592Z",
     "start_time": "2021-10-30T12:34:16.848918Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T01:55:31.112284Z",
     "start_time": "2021-10-30T01:55:31.107096Z"
    }
   },
   "outputs": [],
   "source": [
    "# # this notebook was originally run on linux using Google Chrome\n",
    "\n",
    "# !sudo cp ./chromedriver /usr/bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T16:55:42.191759Z",
     "start_time": "2021-10-30T16:55:42.161686Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_details(item):\n",
    "    \"\"\"Extract the details of an item.\n",
    "    \n",
    "    Argument:\n",
    "        item -- a BeautifulSoup element containing url to the item's page.\n",
    "        \n",
    "    Return:\n",
    "        a dictionary containing all details scrapped for the specified item.\n",
    "    \"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # extract the url\n",
    "    url = item.find('div', 'item-product__box-name').a.get('href')\n",
    "    \n",
    "    driver.get(url)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    results['url'] = url\n",
    "    \n",
    "    # name\n",
    "    results['item_name'] = soup.find('div', {'class': 'box-name__box-product-name'}).h1.text.strip()\n",
    "\n",
    "    # price\n",
    "    price_box = soup.find('div', 'box-info__box-price')\n",
    "    try:\n",
    "        results['special_price'] = price_box.find('p', {'class': 'special-price'}).text[:-2]\n",
    "    except:\n",
    "        results['special_price'] = None\n",
    "    try:\n",
    "        results['old_price'] = price_box.find('p', {'class': 'old-price'}).text[:-2]\n",
    "    except:\n",
    "        results['old_price'] = None\n",
    "    \n",
    "    # versions with different prices\n",
    "    versions_raw = soup.find_all('a', 'item-linked')\n",
    "    versions = {\n",
    "        version.strong.text: version.span.text[:-2] for version in versions_raw\n",
    "    }\n",
    "    results.update(versions)\n",
    "    \n",
    "    # rating\n",
    "    rating_raw = soup.find_all('div', 'item-statistical')\n",
    "    rating = {\n",
    "        (level.find('p', 'number-star').strong.text + 'star'): level.find('p', 'number-percent').text[:-9]\n",
    "        for level in rating_raw\n",
    "    }\n",
    "    results.update(rating)\n",
    "    \n",
    "    # specifications\n",
    "    info_table = soup.find('div', {'id': 'technicalInfoModal'}).find_all('th')\n",
    "\n",
    "    infos = {\n",
    "        info_table[2*i].text: info_table[2*i + 1].text for i in range(len(info_table)//2)\n",
    "    }\n",
    "    results.update(infos)\n",
    "    \n",
    "    # comment count\n",
    "    try:\n",
    "        results['comment_count'] = soup.find('p', {'id': 'total_comment'}).text.split()[3][1:]\n",
    "    except:\n",
    "        results = get_details(item)\n",
    "    \n",
    "    return results\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T14:13:05.757435Z",
     "start_time": "2021-10-30T14:12:20.159491Z"
    }
   },
   "outputs": [],
   "source": [
    "page_url = \"https://cellphones.com.vn/laptop.html\"\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get(page_url)\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# click to the \"show more\" button to get full list of laptops\n",
    "load = True\n",
    "while load:\n",
    "    try:\n",
    "        WebDriverWait(driver, 60).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, 'btn-show-more'))\n",
    "            ).click()        \n",
    "    except:\n",
    "        load = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T23:44:26.059380Z",
     "start_time": "2021-10-30T23:44:26.050957Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'440 laptops found'"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "# scrap the list of laptops\n",
    "results = soup.find_all('div', {\"class\": \"item-product\"})\n",
    "\n",
    "str(len(results)) + ' laptops found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T18:22:07.029584Z",
     "start_time": "2021-10-30T16:56:00.495956Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "records = []\n",
    "\n",
    "for item in results:\n",
    "    record = get_details(item)\n",
    "    records.append(record)\n",
    "    print(record['item_name'], 'details scrapped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T18:22:14.815858Z",
     "start_time": "2021-10-30T18:22:14.784366Z"
    }
   },
   "outputs": [],
   "source": [
    "key_set = set()\n",
    "\n",
    "for record in records:\n",
    "    for k in record.keys():\n",
    "        key_set.add(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-30T18:22:19.060323Z",
     "start_time": "2021-10-30T18:22:18.926694Z"
    }
   },
   "outputs": [],
   "source": [
    "# save records to .csv file\n",
    "file = open('records_cellphones.csv', 'w')\n",
    "writer = csv.DictWriter(file, key_set)\n",
    "writer.writeheader()\n",
    "writer.writerows(records)\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4cv",
   "language": "python",
   "name": "dl4cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
